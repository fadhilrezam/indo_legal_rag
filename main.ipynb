{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watusi Legal Insight\n",
    "\n",
    "### Your FIrst STop for Legal Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install chromadb\n",
    "# !pip install ragas\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import regex as re\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, List, Dict, Annotated\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import NotRequired\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain.schema import Document\n",
    "from langchain_core.messages import AIMessage,HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder)\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "from langchain_chroma import Chroma\n",
    "import chromadb\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import JinaRerank\n",
    "from mistralai import Mistral\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_groq.chat_models import ChatGroq\n",
    "import uuid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents and Convert to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'documents'\n",
    "\n",
    "noise_patterns = [\n",
    "    r'www\\.hukumonline\\.com',\n",
    "    r'\\nMenemukan kesalahan ketik dalam dokumen\\?\\nKlik di sini\\nuntuk perbaikan.'\n",
    "]\n",
    "# pattern = r'(www\\.hukumonline\\.com|\\nMenemukan kesalahan ketik dalam dokumen\\?\\nKlik di sini\\nuntuk perbaikan.)'\n",
    "all_docs = []\n",
    "\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            for noise in noise_patterns:\n",
    "                doc.page_content = re.sub(noise, '', doc.page_content)\n",
    "        \n",
    "        all_docs.extend(docs)\n",
    "\n",
    "full_text = ''.join(page.page_content for page in all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse The Document into Customize Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_title_pattern = r'kitab\\s+undang-undang\\s+hukum\\s+perdata'\n",
    "document_title = re.search(document_title_pattern, full_text, re.IGNORECASE) \n",
    "document_title.group(0) #document_title\n",
    "\n",
    "\n",
    "#Split berdasarkan buku ex:BUKU KETIGA, BUKU KEEMPAT\n",
    "list_buku = []\n",
    "list_bab = []\n",
    "docs = []\n",
    "buku_chunks = re.split(r'(?=BUKU\\s+KE\\w+)', full_text)\n",
    "for buku in buku_chunks:\n",
    "    buku_pattern = r'(BUKU\\s+KE\\w+).*'\n",
    "    buku_match = re.search(buku_pattern, buku, re.DOTALL)\n",
    "    # if not re.search(buku_pattern, buku, re.DOTALL):\n",
    "    if not buku_match:\n",
    "        continue\n",
    "    list_buku.append(buku_match.group(1))\n",
    "    # print(f'{buku_match.group(1)}')\n",
    "\n",
    "    #Split berdasarkan bab\n",
    "    bab_chunks = re.split(r'(?=BAB\\s+[IVXLCDM]+[A-Z]?)', buku)\n",
    "    for bab in bab_chunks:\n",
    "        bab_pattern = r'(BAB\\s+[IVXLCDM]+[A-Z]?)\\s*\\n(.*?)\\s*\\n(.*)'\n",
    "        # bab_pattern = r'BAB\\s+([IVXLCDM]+)\\s*\\n((?:.(?!\\nPasal\\s*\\d))+)'\n",
    "        bab_match = re.search(bab_pattern, bab, re.DOTALL)\n",
    "        if not bab_match:\n",
    "            continue\n",
    "        bab_number = bab_match.group(1).strip()\n",
    "        bab_title = bab_match.group(2).strip()\n",
    "        bab_title = re.sub(r'\\s*\\n\\s*',' ', bab_title)\n",
    "        # print(f'{buku_match.group(1)} -> {bab_match.group(1)}: {bab_match.group(2)}')\n",
    "\n",
    "        #Proses pasal yang ada dalam 'BAGIAN' di setiap BABnya\n",
    "        if 'BAGIAN' in bab:\n",
    "            bagian_chunks = re.split(r'(?=BAGIAN\\s+[0-9]+)', bab)\n",
    "            for bagian in bagian_chunks:\n",
    "                bagian_pattern = r'(BAGIAN\\s+[0-9]+)\\s*\\n*(.*?)\\s*\\n\\s*\\n(.*)'\n",
    "                bagian_match = re.search(bagian_pattern, bagian, re.DOTALL)\n",
    "                if not bagian_match:\n",
    "                    continue\n",
    "                bagian_number = bagian_match.group(1).strip()\n",
    "                bagian_title=bagian_match.group(2).strip()\n",
    "                # print(f'{buku_match.group(1)} -> {bab_number}: {bab_title} -> {bagian_number}: {bagian_title}')\n",
    "\n",
    "                # Proses Pasal\n",
    "                pasal_chunks = re.split(r'(?=Pasal\\s+[0-9]+[a-z]?)', bagian, re.DOTALL)\n",
    "                for pasal in pasal_chunks:\n",
    "                    pasal_pattern = r'(Pasal\\s+[0-9]+[a-z]?)\\s*\\n(.*)'\n",
    "                    pasal_match = re.search(pasal_pattern, pasal, re.DOTALL)\n",
    "                    if not pasal_match:\n",
    "                        continue\n",
    "                    pasal_number = pasal_match.group(1).strip()\n",
    "                    pasal_content = pasal_match.group(2).strip()\n",
    "                \n",
    "                    doc = Document(\n",
    "                        page_content =\n",
    "                            f\"Dokumen: {document_title.group(0)}\\n\"\n",
    "                            f\"Bab: {bab_number} - {bab_title}\\n\"\n",
    "                            f\"Bagian: {bagian_number} - {bagian_title}\\n\"\n",
    "                            f\"Pasal: {pasal_number}\\n\"\n",
    "                            f\"Isi Pasal: {pasal_content}\",\n",
    "                        metadata = {\n",
    "                        'document_type': 'legal_document',\n",
    "                        'document_title': document_title.group(0) + ' ' + buku_match.group(1),\n",
    "                        'bab_number': bab_number,\n",
    "                        'bab_title': bab_title,\n",
    "                        'bagian': bagian_number,\n",
    "                        'bagian_title': bagian_title,\n",
    "                        'pasal': pasal_number}\n",
    "                    )\n",
    "                    docs.append(doc)\n",
    "\n",
    "        # Proses Pasal Tanpa Bagian\n",
    "        else:\n",
    "            pasal_chunks = re.split(r'(?=Pasal\\s+[0-9]+[a-z]?)', bab, re.DOTALL)\n",
    "            for pasal in pasal_chunks:\n",
    "                    pasal_pattern = r'(Pasal\\s+[0-9]+[a-z]?)\\s*\\n(.*)'\n",
    "                    pasal_match = re.search(pasal_pattern, pasal, re.DOTALL)\n",
    "                    if not pasal_match:\n",
    "                        continue\n",
    "                    pasal_number = pasal_match.group(1).strip()\n",
    "                    pasal_content = pasal_match.group(2).strip()\n",
    "                    doc = Document(\n",
    "                        page_content =\n",
    "                            f\"Dokumen: {document_title.group(0)}\\n\"\n",
    "                            f\"Bab: {bab_number} - {bab_title}\\n\"\n",
    "                            f\"Bagian: {bagian_number} - {bagian_title}\\n\"\n",
    "                            f\"Pasal: {pasal_number}\\n\"\n",
    "                            f\"Isi Pasal: {pasal_content}\",\n",
    "                        metadata = {\n",
    "                        'document_type': 'legal_document',\n",
    "                        'document_title': document_title.group(0) + ' ' + buku_match.group(1),\n",
    "                        'bab_number': bab_number,\n",
    "                        'bab_title': bab_title,\n",
    "                        'bagian': '',\n",
    "                        'bagian_title': '',\n",
    "                        'pasal': pasal_number}\n",
    "                    )\n",
    "                    docs.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Documents, Store in Vector DB and Rerank with Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load api key\n",
    "with open('credentials.json', 'r')as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "mistral_api = data['mistral_api']\n",
    "jina_api = data['jina_api']\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#load embedding model\n",
    "model_name = 'mistral-embed'\n",
    "embedding_model = MistralAIEmbeddings(\n",
    "    model = model_name,\n",
    "    api_key=mistral_api)\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#store / load document from chroma db\n",
    "vector_store_mistral = Chroma(\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory='.\\pipeline\\chroma_db',\n",
    "    collection_name = 'kuhperdata_mistral'   \n",
    ")\n",
    "# vector_store.add_documents(docs) -> Run this script to add document to vector db\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#set vector db as retriver with search type similarity and get 'top 5' documents\n",
    "retriever = vector_store_mistral.as_retriever(\n",
    "    search_type = 'similarity',\n",
    "    k= 5)\n",
    "\n",
    "#rerank the documents with the help of Cross Encoder model (in my case I'm utilazing Jina Reranker model)\n",
    "compressor = JinaRerank(jina_api_key = jina_api)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever)\n",
    "#------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Run code below to identified what vector db (collection) that available in the local storage\n",
    "# client = chromadb.PersistentClient(path = './pipeline/chroma_db/')\n",
    "# client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_retrieval(retriever, query, expected_content=None):\n",
    "    \"\"\"Debug retrieval dengan eye-balling results\"\"\"\n",
    "    results = retriever.similarity_search_with_score(query, k=3)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results):\n",
    "        print(f\"Rank {i+1} | Score: {score:.4f}\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "        if expected_content and expected_content.lower() in doc.page_content.lower():\n",
    "            print(\"✅ CONTAINS EXPECTED CONTENT!\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Test\n",
    "# debug_retrieval(vector_store_mistral, \"Pasal 1234\", \"Perikatan ditujukan untuk memberikan sesuatu, untuk berbuat sesuatu, atau untuk tidak berbuat sesuatu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LLM Model\n",
    "\n",
    "1. Mistral LLM for RAG Chain -> *gemma model as alternative with local environment*\n",
    "2. Gemini LLM for RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credentials.json', 'r')as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "mistral_api = data['mistral_api']\n",
    "open_route_api = data['open_route_api']\n",
    "groq_api = data['groq_api']\n",
    "\n",
    "#load gemma model\n",
    "ollama_llm = OllamaLLM(\n",
    "    model = 'gemma3:1b',\n",
    "    temperature = 0.2)\n",
    "\n",
    "#load mistral model\n",
    "mistral_llm = ChatMistralAI(\n",
    "    # model = 'Hermes-2-Pro-Mistral-7B',\n",
    "    model = 'mistral-small-2506',\n",
    "    api_key=mistral_api,\n",
    "    temperature = 0.1\n",
    ")\n",
    "\n",
    "#load gemini model\n",
    "openrouter_llm = ChatOpenAI(\n",
    "    model = 'google/gemini-2.0-flash-exp:free',\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key = open_route_api\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RAG Chain Utilizing Langchain and Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OverallState(TypedDict):\n",
    "    input: str\n",
    "    retrieved_docs: List[Document]\n",
    "    prompt: str\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    llm: object\n",
    "    nama: NotRequired[str]\n",
    "    domisili: NotRequired[str]\n",
    "\n",
    "\n",
    "def retriever_node(state:OverallState) -> dict:\n",
    "    input = state['input']\n",
    "    # docs = retriever.get_relevant_documents(input)\n",
    "    docs = [doc for doc in compression_retriever.get_relevant_documents(input)]\n",
    "\n",
    "    return({'input':input, 'retrieved_docs':docs})\n",
    "\n",
    "def merger_node(state:OverallState) -> dict:\n",
    "    # user_input = state['input']\n",
    "    context = '\\n'.join([doc.page_content for doc in state['retrieved_docs']])\n",
    "    system_prompt = f\"\"\"\n",
    "    Kamu adalah seorang ahli hukum. Jawab pertanyaan user berdasarkan Konteks: \n",
    "    {context}\n",
    "\n",
    "    ### Instruksi:\n",
    "    - Jawab selalu dalam **bahasa Indonesia** dan **output harus di bawah 50 kata**\n",
    "    - **Jawaban langsung ke inti**\n",
    "    - Jika pertanyaannya meminta isi pasal (misalnya menyebut \"Pasal 1320\", \"bunyi Pasal\", dsb), kutip isi pasal yang diminta **secara langsung** dari konteks.\n",
    "    - Jika pertanyaannya berupa kasus hukum (contoh: pelanggaran perjanjian kerja sama, wanprestasi, dll), berikan pasal **yang relevan saja** dari konteks, jangan mengarang.\n",
    "    - Jika informasi tidak ditemukan dalam konteks, katakan bahwa **informasi tidak tersedia** atau **ajukan pertanyaan untuk meminta kejelasan ke user**\n",
    "    \n",
    "    ### Deteksi Lanjutan (Sinyal Eskalasi):\n",
    "    Jika user menunjukkan salah satu dari hal berikut, barulah berikan **kontak pengacara**:\n",
    "    1. Menyatakan **bingung secara hukum**, misal: “Saya tidak tahu harus bagaimana”, “Tolong bantu saya”\n",
    "    2. Menyatakan ingin **dibantu secara langsung**\n",
    "    3. Menyebut butuh bantuan **membuat/review dokumen**\n",
    "    4. Menunjukkan **keputusasaan/kebingungan yang jelas**\n",
    "    5. Menyatakan ingin **berkonsultasi lebih lanjut**\n",
    "    6. Jangan berikan tawaran bantuan hukum **di awal percakapan**\n",
    "\n",
    "    ### Jika Sinyal Terdeteksi:\n",
    "    Tambahkan di akhir jawaban:\n",
    "    \"Jika Anda membutuhkan bantuan lebih lanjut, silakan hubungi kami melalui email [Watusi Legal Insight](contact@watusilegalinsight.com) atau WhatsApp 0812-3456-7890. Tim kami akan menghubungi Anda setelah verifikasi data dilakukan.\"\n",
    "\n",
    "    **Jangan berikan kontak jika belum ada sinyal di atas.**\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate([\n",
    "        ('system', system_prompt),\n",
    "        MessagesPlaceholder(variable_name = 'messages'),\n",
    "        ('human', state['input'])\n",
    "        ])\n",
    "    # print(prompt)\n",
    "    return({\n",
    "        'prompt':prompt,\n",
    "        'messages':[HumanMessage(content = state['input'])]})\n",
    "\n",
    "def output_node(state:OverallState) -> dict:\n",
    "    qa_chain = state['prompt'] | state['llm']\n",
    "    result = qa_chain.invoke({'messages': state['messages']})\n",
    "    new_messasges = state['messages'] + [result]\n",
    "    # print(f\"[DEBUG] Model result: {result}\")\n",
    "    return ({'messages': new_messasges})\n",
    "\n",
    "# def summary_node (state: OverallState) -> dict:\n",
    "    \n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "builder.add_node('retriever', retriever_node)\n",
    "builder.add_node('merger', merger_node)\n",
    "builder.add_node('output', output_node)\n",
    "builder.add_edge(START, 'retriever')\n",
    "builder.add_edge('retriever', 'merger')\n",
    "builder.add_edge('merger', 'output')\n",
    "builder.add_edge('output', END)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "store = InMemoryStore()\n",
    "graph_memory = builder.compile(checkpointer=checkpointer, store = store)\n",
    "graph = builder.compile()\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot Function to Retrieve Question (chat bot with memory and not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_bot_with_memory(question:str, llm = None):\n",
    "    if llm is None:\n",
    "        llm = mistral_llm\n",
    "        final_result = None\n",
    "\n",
    "        for chunk in graph_memory.stream({'input': question, 'llm': llm}, config, stream_mode = 'values'):\n",
    "            final_result = chunk\n",
    "        for i in range(len(final_result['messages'])):\n",
    "            final_result['messages'][i].pretty_print()\n",
    "        return final_result\n",
    "\n",
    "def chat_bot(question: str, llm = None):\n",
    "    if llm is None:\n",
    "        llm = mistral_llm\n",
    "    result = graph.invoke({'input':question, 'llm':llm})\n",
    "    return result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = 'bunyi pasal 1234'\n",
    "# result = graph.invoke({'input':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Kantor saya tidak membayarkan upah selama 3 bulan terakhir, bisa dikenakan pasal berapa aja ?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Pasal 1602q KUHPer: Jika upah tidak dibayar paling lambat hari kerja ketiga setelah hari pembayaran, buruh berhak atas tambahan upah 5% per hari hingga hari ke-8 dan 1% per hari setelahnya, maksimal 50% dari jumlah upah.\n"
     ]
    }
   ],
   "source": [
    "question = 'Kantor saya tidak membayarkan upah selama 3 bulan terakhir, bisa dikenakan pasal berapa aja ?'\n",
    "final_result = chat_bot_with_memory(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Summarize Conversation (Based on Chatbot with Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'User tidak dibayar upah 3 bulan, AI menjelaskan pasal 1602q KUHPer tentang tambahan upah jika upah tidak dibayar tepat waktu. Tidak ada indikasi user butuh bantuan hukum lanjutan.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_summarizer(conversation:dict):\n",
    "    conversation = \"\"\n",
    "    for msg in final_result['messages']:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            role = 'user'\n",
    "        else:\n",
    "            role = 'ai'\n",
    "\n",
    "        conversation += f\"{role}: {msg.content}\\n\\n\"\n",
    "\n",
    "    system_prompt =\"\"\"Buat ringkasan percakapan ini maksimal 100 kata. \n",
    "            Jelaskan inti permasalahan user, apa jawaban dari AI, dan apakah ada indikasi bahwa user butuh bantuan hukum lanjutan atau tidak to the point langsung ringkas tanpa header apapun.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", conversation)])\n",
    "\n",
    "    qa_chain = prompt | mistral_llm\n",
    "\n",
    "    chat_sumamry = qa_chain.invoke({}).content\n",
    "    return chat_sumamry\n",
    "\n",
    "conv_summarizer(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rag Evaluation with RAGAS Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Provider returned error', 'code': 429, 'metadata': {'raw': 'google/gemini-2.0-flash-exp:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations', 'provider_name': 'Google'}}, 'user_id': 'user_30JgS4W6yJ9QzMABCrko9J1YoDx'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m contexts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[1;32m---> 23\u001b[0m     answers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mchat_bot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenrouter_llm\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     24\u001b[0m     contexts\u001b[38;5;241m.\u001b[39mappend([doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m compression_retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(question)])\n\u001b[0;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     27\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m'\u001b[39m: questions,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m: answers,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrieved_contexts\u001b[39m\u001b[38;5;124m'\u001b[39m: contexts,\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m'\u001b[39m: ground_truths})\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mchat_bot\u001b[1;34m(question, llm)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     llm \u001b[38;5;241m=\u001b[39m mistral_llm\n\u001b[1;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2844\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[0m\n\u001b[0;32m   2841\u001b[0m chunks: \u001b[39mlist\u001b[39m[\u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any] \u001b[39m|\u001b[39m Any] \u001b[39m=\u001b[39m []\n\u001b[0;32m   2842\u001b[0m interrupts: \u001b[39mlist\u001b[39m[Interrupt] \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 2844\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream(\n\u001b[0;32m   2845\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   2846\u001b[0m     config,\n\u001b[0;32m   2847\u001b[0m     stream_mode\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mupdates\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mvalues\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m   2848\u001b[0m     \u001b[39mif\u001b[39;49;00m stream_mode \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mvalues\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m   2849\u001b[0m     \u001b[39melse\u001b[39;49;00m stream_mode,\n\u001b[0;32m   2850\u001b[0m     print_mode\u001b[39m=\u001b[39;49mprint_mode,\n\u001b[0;32m   2851\u001b[0m     output_keys\u001b[39m=\u001b[39;49moutput_keys,\n\u001b[0;32m   2852\u001b[0m     interrupt_before\u001b[39m=\u001b[39;49minterrupt_before,\n\u001b[0;32m   2853\u001b[0m     interrupt_after\u001b[39m=\u001b[39;49minterrupt_after,\n\u001b[0;32m   2854\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m   2855\u001b[0m ):\n\u001b[0;32m   2856\u001b[0m     \u001b[39mif\u001b[39;49;00m stream_mode \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mvalues\u001b[39;49m\u001b[39m\"\u001b[39;49m:\n\u001b[0;32m   2857\u001b[0m         \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(chunk) \u001b[39m==\u001b[39;49m \u001b[39m2\u001b[39;49m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2534\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[0m\n\u001b[0;32m   2532\u001b[0m \u001b[39mfor\u001b[39;00m task \u001b[39min\u001b[39;00m loop\u001b[39m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2533\u001b[0m     loop\u001b[39m.\u001b[39moutput_writes(task\u001b[39m.\u001b[39mid, task\u001b[39m.\u001b[39mwrites, cached\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 2534\u001b[0m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m runner\u001b[39m.\u001b[39;49mtick(\n\u001b[0;32m   2535\u001b[0m     [t \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m loop\u001b[39m.\u001b[39;49mtasks\u001b[39m.\u001b[39;49mvalues() \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m t\u001b[39m.\u001b[39;49mwrites],\n\u001b[0;32m   2536\u001b[0m     timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_timeout,\n\u001b[0;32m   2537\u001b[0m     get_waiter\u001b[39m=\u001b[39;49mget_waiter,\n\u001b[0;32m   2538\u001b[0m     schedule_task\u001b[39m=\u001b[39;49mloop\u001b[39m.\u001b[39;49maccept_push,\n\u001b[0;32m   2539\u001b[0m ):\n\u001b[0;32m   2540\u001b[0m     \u001b[39m# emit output\u001b[39;49;00m\n\u001b[0;32m   2541\u001b[0m     \u001b[39myield from\u001b[39;49;00m _output(\n\u001b[0;32m   2542\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[39m.\u001b[39;49mget, queue\u001b[39m.\u001b[39;49mEmpty\n\u001b[0;32m   2543\u001b[0m     )\n\u001b[0;32m   2544\u001b[0m loop\u001b[39m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langgraph\\pregel\\runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    160\u001b[0m t \u001b[39m=\u001b[39m tasks[\u001b[39m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     run_with_retry(\n\u001b[0;32m    163\u001b[0m         t,\n\u001b[0;32m    164\u001b[0m         retry_policy,\n\u001b[0;32m    165\u001b[0m         configurable\u001b[39m=\u001b[39;49m{\n\u001b[0;32m    166\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[0;32m    167\u001b[0m                 _call,\n\u001b[0;32m    168\u001b[0m                 weakref\u001b[39m.\u001b[39;49mref(t),\n\u001b[0;32m    169\u001b[0m                 retry_policy\u001b[39m=\u001b[39;49mretry_policy,\n\u001b[0;32m    170\u001b[0m                 futures\u001b[39m=\u001b[39;49mweakref\u001b[39m.\u001b[39;49mref(futures),\n\u001b[0;32m    171\u001b[0m                 schedule_task\u001b[39m=\u001b[39;49mschedule_task,\n\u001b[0;32m    172\u001b[0m                 submit\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubmit,\n\u001b[0;32m    173\u001b[0m             ),\n\u001b[0;32m    174\u001b[0m         },\n\u001b[0;32m    175\u001b[0m     )\n\u001b[0;32m    176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommit(t, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langgraph\\pregel\\retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[39m.\u001b[39mwrites\u001b[39m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[39m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m task\u001b[39m.\u001b[39;49mproc\u001b[39m.\u001b[39;49minvoke(task\u001b[39m.\u001b[39;49minput, config)\n\u001b[0;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m ParentCommand \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[39m# run in context\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[39mwith\u001b[39;00m set_config_context(config, run) \u001b[39mas\u001b[39;00m context:\n\u001b[1;32m--> 623\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39;49mrun(step\u001b[39m.\u001b[39;49minvoke, \u001b[39minput\u001b[39;49m, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    624\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    625\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39minvoke(\u001b[39minput\u001b[39m, config)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m         run_manager\u001b[39m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    376\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 377\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecurse \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    379\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39minvoke(\u001b[39minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[8], line 60\u001b[0m, in \u001b[0;36moutput_node\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moutput_node\u001b[39m(state:OverallState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m     59\u001b[0m     qa_chain \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m|\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 60\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     new_messasges \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [result]\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# print(f\"[DEBUG] Model result: {result}\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3046\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3044\u001b[0m                 input_ \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, input_, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   3045\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3046\u001b[0m                 input_ \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39mrun(step\u001b[39m.\u001b[39minvoke, input_, config)\n\u001b[0;32m   3047\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[0;32m   3048\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m    384\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39minvoke\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    391\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[0;32m    392\u001b[0m     config \u001b[39m=\u001b[39m ensure_config(config)\n\u001b[0;32m    393\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mChatGeneration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m--> 395\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[0;32m    396\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[0;32m    397\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    398\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    399\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    400\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    401\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    402\u001b[0m             run_id\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mrun_id\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    403\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    404\u001b[0m         )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    405\u001b[0m     )\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:980\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[39m@override\u001b[39m\n\u001b[0;32m    972\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    973\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    978\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    979\u001b[0m     prompt_messages \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_messages() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_messages, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:799\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(input_messages):\n\u001b[0;32m    797\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 799\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_with_cache(\n\u001b[0;32m    800\u001b[0m                 m,\n\u001b[0;32m    801\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[0;32m    802\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[i] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    803\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    804\u001b[0m             )\n\u001b[0;32m    805\u001b[0m         )\n\u001b[0;32m    806\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    807\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1045\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     result \u001b[39m=\u001b[39m generate_from_stream(\u001b[39miter\u001b[39m(chunks))\n\u001b[0;32m   1044\u001b[0m \u001b[39melif\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1045\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[0;32m   1046\u001b[0m         messages, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1131\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     generation_info \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mdict\u001b[39m(raw_response\u001b[39m.\u001b[39mheaders)}\n\u001b[0;32m   1130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpayload)\n\u001b[0;32m   1132\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 287\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1087\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   1045\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcreate\u001b[39m(\n\u001b[0;32m   1046\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1085\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m   1086\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1087\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[0;32m   1088\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1089\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[0;32m   1090\u001b[0m             {\n\u001b[0;32m   1091\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[0;32m   1092\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[0;32m   1093\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m: audio,\n\u001b[0;32m   1094\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[0;32m   1095\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[0;32m   1096\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[0;32m   1097\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[0;32m   1098\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[0;32m   1099\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_completion_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_completion_tokens,\n\u001b[0;32m   1100\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[0;32m   1101\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m: metadata,\n\u001b[0;32m   1102\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodalities\u001b[39;49m\u001b[39m\"\u001b[39;49m: modalities,\n\u001b[0;32m   1103\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[0;32m   1104\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[0;32m   1105\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m\"\u001b[39;49m: prediction,\n\u001b[0;32m   1106\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[0;32m   1107\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mreasoning_effort\u001b[39;49m\u001b[39m\"\u001b[39;49m: reasoning_effort,\n\u001b[0;32m   1108\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[0;32m   1109\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[0;32m   1110\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mservice_tier\u001b[39;49m\u001b[39m\"\u001b[39;49m: service_tier,\n\u001b[0;32m   1111\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[0;32m   1112\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstore\u001b[39;49m\u001b[39m\"\u001b[39;49m: store,\n\u001b[0;32m   1113\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[0;32m   1114\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[0;32m   1115\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[0;32m   1116\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[0;32m   1117\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[0;32m   1118\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[0;32m   1119\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[0;32m   1120\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[0;32m   1121\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mweb_search_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: web_search_options,\n\u001b[0;32m   1122\u001b[0m             },\n\u001b[0;32m   1123\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParamsStreaming\n\u001b[0;32m   1124\u001b[0m             \u001b[39mif\u001b[39;49;00m stream\n\u001b[0;32m   1125\u001b[0m             \u001b[39melse\u001b[39;49;00m completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParamsNonStreaming,\n\u001b[0;32m   1126\u001b[0m         ),\n\u001b[0;32m   1127\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[0;32m   1128\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m   1129\u001b[0m         ),\n\u001b[0;32m   1130\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[0;32m   1131\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1132\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[0;32m   1133\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\openai\\_base_client.py:1256\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mpost\u001b[39m(\n\u001b[0;32m   1243\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1244\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1252\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m   1253\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[0;32m   1254\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[0;32m   1255\u001b[0m     )\n\u001b[1;32m-> 1256\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_langgraph\\Lib\\site-packages\\openai\\_base_client.py:1044\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m   1043\u001b[0m         log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1044\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39massert\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mcould not resolve response (should never happen)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Provider returned error', 'code': 429, 'metadata': {'raw': 'google/gemini-2.0-flash-exp:free is temporarily rate-limited upstream. Please retry shortly, or add your own key to accumulate your rate limits: https://openrouter.ai/settings/integrations', 'provider_name': 'Google'}}, 'user_id': 'user_30JgS4W6yJ9QzMABCrko9J1YoDx'}"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "docs_inference = random.sample(docs, 20)\n",
    "\n",
    "questions = []\n",
    "ground_truths = []\n",
    "\n",
    "dataset = []\n",
    "for doc in docs_inference:\n",
    "    question = f'Bunyi {doc.metadata.get(\"pasal\")} adalah ?'\n",
    "    # ground_truth = re.sub('\\nIsi Pasal:', ',', re.findall(r'(Pasal\\s\\d+\\w*\\n.*)',doc.page_content)[0])\n",
    "    ground_truth = doc.page_content\n",
    "    \n",
    "    questions.append(question)\n",
    "    ground_truths.append(ground_truth)\n",
    "\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "for question in questions:\n",
    "    answers.append(chat_bot(question, openrouter_llm))\n",
    "    contexts.append([doc.page_content for doc in compression_retriever.get_relevant_documents(question)])\n",
    "\n",
    "data = (\n",
    "    {'user_input': questions,\n",
    "    'response': answers,\n",
    "    'retrieved_contexts': contexts,\n",
    "    'reference': ground_truths})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 15/15 [00:27<00:00,  1.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'context_recall': 1.0000, 'faithfulness': 0.9000, 'factual_correctness(mode=f1)': 0.5480}"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (faithfulness, answer_relevancy, context_recall, context_precision)\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(mistral_llm)\n",
    "result = evaluate(dataset=dataset,metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_recall': 1.0000, 'faithfulness': 0.9000, 'factual_correctness(mode=f1)': 0.5480}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.13 ('llm_langgraph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "563f4e1139af12a3e466c92be8b5f61db6fc37b4947f50811b94e9833745ec63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
